---
layout: default
title: Random Matrix Theory Notes
description: Capturing the many ideas and trying to draw them together
tags:
- Random Matrix Theory
- Blog
---
# Random Notes about Random Matrix Theory

Alex sends me brain dumps via WhatsApp. It's too much to parse.

These brain dumps date back over six months worth of text messages.

It's worse than doing a PhD because it's somebody else's wandering notes from the internets.

## In the beginning

LLMs are living in the tangent space to the underlying graph of human ~~reasoning~~ languange.
If you're in a very dense areas of the input space you get good answers, but if you're further out you get statistical language salad.

### Hypothesis:
> Most of MLOps is solving the wrong problem.
> There are only two real problems,
> 1. Ensuring a model can smoothly go into an identically-performing production harness, and
> 2. Modeling performance over time.
> Model iteration is a waste of time.

**Spectral analysis** might yield a _universal standard for model performance_.
The right kind of spectral analysis may yield information about the model structure that will
imply or guarantee how the training and inference will perform.

During Neural Network (NN) training, the model accuracy is measure (by epochs) against the training and the test set.
Then, in production, the NN model accuracy is often not measured - it is simply used for inference or prediction,
e.g. predicting the words that should go in a response. Model accuracy _in production_ is only captured if you can get
the ground truth and measure the distance between the model response and the ground truth.

There is an observed phenomena around over-training a model. In fact, far beyond over-training, there is another point where
model accuracy improves (only for some models) well beyond the normal point where training would have stopped. In this region
the LLMs appear to get really good at building sentences and appear to really understand concepts. But are they _understanding_
anything? Or are they just making great sentences in response to questions in a well-understood part of the human language opus?

Circling back, a trained NN encodes a flow of information. During model training, we are tuning the weights that allow the information to flow.
We want to know if there are spectral or topological properties of the model that can guarantee that information can flow. In other words,
can we guarantee that we can train this model to high accuracy? Are there spectral or topological features that we can identify
**before training**?

### Neural Network Training Regimes
Is there a geometry of the training process, or the initialisation of training, that we can apply a numerical or spectral analysis.
The goal is to have an analysis that tells you what hyperparameters, or what test/train split, or what number of epochs should work for your model, and your training data.

There are limitations to any _training data_. Even if you think of the simplest regression model, $\hat{y} = \beta_0 + \beta_1 x$,
you know that the quality and amount of training data control the amount of predictive power you can expect from your model.

So, we just want to do that, but for huge models - like LLMs.

**These notes bring us to 3-July-2025 in my text convo**


### Definitions and Ideas

<dl>
<dt>Spectral Analysis</dt>
<dd>
  If you examine the spectra of a matrix (of model weights) then you can say something about the convergence proporties of that model or matrix.
</dd>
<dt>Free convolution</dt>
<dd>Green</dd>
<dt>Non-commutative probability</dt>
<dd>Green</dd>

<dt>Color</dt>
<dd>Green</dd>
</dl>
